Gizmo {
 onCreate "import sys\nthis = nuke.thisNode()\ninference = nuke.toNode(f\"\{this.name()\}.Inference1\")\nthis\[\"gpuName\"].setValue(inference\[\"gpuName\"].value())\nthis\[\"channelsIn\"].setValue(\"rgba.red, rgba.green, rgba.blue\")\ninference.forceValidate()\nis_enabled = inference\[\"modelFile\"].enabled()\nif (sys.platform.lower() == \"darwin\") and (not inference\[\"useGPUIfAvailable\"].enabled()): this\[\"useGPUIfAvailable\"].setValue(False), this\[\"useGPUIfAvailable\"].setEnabled(False)\nif not is_enabled:\n    for k in this.knobs(): this\[k].setEnabled(False)\nthis\[\"halfPrecision\"].setVisible(this\[\"useGPUIfAvailable\"].enabled())"
 onDestroy "nuke.thisNode()\[\"knobChanged\"].setValue(\"\")"
 knobChanged "this = nuke.thisNode()\nthis\[\"halfPrecision\"].setVisible(this\[\"useGPUIfAvailable\"].value())"
 addUserKnob {20 Parameters}
 addUserKnob {26 localGPU l "Local GPU:" T ""}
 addUserKnob {26 gpuName l "" -STARTLINE T "NVIDIA GeForce RTX 3090"}
 addUserKnob {6 useGPUIfAvailable l "Use GPU if available" t "Select this to render on the <b>Local GPU</b>, if available.\n\nYou can select this even if no GPU is currently available on your machine. The GPU will then be used whenever the script is opened on a machine which does have a GPU available. You should also select this if you wish to render from the command line with the <b>--gpu</b> option.\n\nIf this node requires full frames from its inputs, and is therefore unable to reduce its memory overhead, it will fall back to the CPU if an attempt to render a frame on the GPU fails due to lack of memory. When this occurs, a warning message will be printed to the console." +STARTLINE}
 useGPUIfAvailable true
 addUserKnob {6 halfPrecision l "Optimize for Speed and Memory" t "Whether to process at half float precision. This speeds up execution and enables the processing of larger images, however there is the risk of artifacts with some trained models." +STARTLINE}
 addUserKnob {26 ""}
 addUserKnob {26 channelsIn l "Channels In:" T "rgba.red, rgba.green, rgba.blue"}
 addUserKnob {26 ""}
 addUserKnob {41 optimize_speed l Fast T Inference1.optimize_speed}
 addUserKnob {41 L l "Ressize Computation" T Inference1.L}
 addUserKnob {26 "" l " "}
 addUserKnob {26 "" +STARTLINE}
 addUserKnob {26 _1 l "" +STARTLINE T "<span><b>Original Model </b> - <a href=\"https://github.com/hkchengrex/CascadePSP\" style=\"color:#666;text-decoration: none;\">CascadePSP</a></span>"}
 addUserKnob {26 _2 l "" +STARTLINE T "<span><b>Ported to cattery by </b> - <a href=\"https://www.linkedin.com/in/pcpedrocabrera/\" style=\"color:#666;text-decoration: none;\">Pedro Cabrera</a></span> \n<span><b>Part of the collection </b> - <a href=\"https://github.com/pedroCabrera/CatteryNodes\" style=\"color:#666;text-decoration: none;\">CatteryNodes</a></span>"}
}
 Input {
  inputs 0
  name Input1
  xpos -150
  ypos 67
 }
set N109e6400 [stack 0]
push $N109e6400
 Inference {
  useGPUIfAvailable {{parent.useGPUIfAvailable}}
  modelFile "\[lsearch -inline \[plugins -all cascade_psp.cat] *.cat]"
  halfPrecision {{parent.halfPrecision}}
  serialiseKnob {optimize_speed:false;L:800;}
  name Inference1
  selected true
  xpos -150
  ypos 253
 }
 Output {
  name Output1
  xpos -150
  ypos 423
 }
 Viewer {
  inputs 2
  frame 10
  frame_range 36-36
  name Viewer1
  xpos 130
  ypos 423
 }
end_group
